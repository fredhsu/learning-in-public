<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>linear-algebra</title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
</head>
<body>
<h1 id="linear-algebra-notes">Linear Algebra notes</h1>
<h2 id="vector">Vector</h2>
<p>Vectors are a fundamental concept in linear algebra and can have
multiple interpretations from physics, to computer science, and
mathematics. For our purposes we will use assume they represent a line
that begins at the origin, and goes to the point represented by the
vector. They are represented<span
class="sidenote-wrapper"><label for="sn-0" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-0" class="margin-toggle"/><span
class="sidenote">I am using Pandoc and LaTeX to render some of this
math. In this case pmatrix, bmatrix Bmatrix give parenthesis, brackets,
braces matrix styles are useful latex options.<br />
<br />
</span></span> like this:</p>
<p><span class="math inline">\(\begin{bmatrix} 1 \\ 2
\end{bmatrix}\)</span></p>
<p>The primary vector operations are:</p>
<ul>
<li>vector addition - combining vectors end to end</li>
<li>multiplication by a scalar - scales the vector</li>
</ul>
<p>A vector exists outside of its coordinates and is not the same as its
coordinates Vectors can represent different coordinates in different
coordinate spaces through [<a href="#linear-transformation">linear
transformation</a>].</p>
<h2 id="basis-vectors">Basis Vectors</h2>
<p>Basis vectors of a [<a href="#vector-space">Vector Space</a>] are a
set of [[Linearly Independent]] vectors that span the full space. Since
a vector space can have infinitely many vectors, using the basis allows
us to succinctly define and work with the VS.</p>
<p>Given S subset of V, S is basis of V if</p>
<ul>
<li>S is LI</li>
<li>Span(S) = V</li>
<li>The elements of S are basis vectors</li>
<li>MoML pg 57</li>
</ul>
<h3 id="orthormal-basis-onb">Orthormal Basis (ONB)</h3>
<p>Basis vectors that are orthonormal (orthogonal and have length 1)</p>
<ul>
<li>[<a href="#inner-product">inner product</a>] is 0 : &lt;b_i, b_j&gt;
= 0 for i != j (orthogonal),</li>
<li>&lt;b_i, b_i&gt; = 1 ; have length = 1</li>
</ul>
<h2 id="matrix">Matrix</h2>
<p>A matrix can be used to represent a series of linear equations. For
example given the following linear equations:</p>
<p><span class="math display">\[
x + 2y = 5
3x + 4y = 11
\]</span></p>
<p>This becomes:</p>
<p><span class="math display">\[
\begin{bmatrix}
1 &amp; 2 &amp; 5 \\
3 &amp; 4 &amp; 11 \\
\end{bmatrix}
\]</span></p>
<p>Any matrix can be geometrically viewed as a [<a
href="#linear-transformation">Linear Transformation</a>]</p>
<h3 id="orthogonal-matrix">Orthogonal Matrix</h3>
<ul>
<li>Has columns that are [[orthnormal]], resulting in A^(-1) = A^T,
which makes calculating the inverse efficient</li>
<li>Results in a rotation when viewed as a linear transformation, the
transformation is done relative to an [[orthonormal basis]]</li>
</ul>
<h2 id="linear-combination">Linear Combination</h2>
<p>A combination of scaling (multiplying) or adding vectors</p>
<h2 id="linear-transformation">Linear Transformation</h2>
<p>By multiplying a vector by a matrix, we can perform transformations
of the vector. So geometrically a matrix represents a linear transform
of a vector.</p>
<p>Linear Transformations have the key properties:</p>
<ol type="1">
<li>Lines remain lines, they don’t become curves</li>
<li>Origin stays the same</li>
<li>Grid lines stay equally spaced and parallel</li>
</ol>
<p>The transformation can be described in terms of how the basis vector
changes.</p>
<ul>
<li><a
href="https://claude.ai/public/artifacts/ebfef9fb-c08b-48ca-a9ed-9ec68ef6ba6b">Interactive
Tool</a></li>
<li><a href="https://www.youtube.com/watch?v=kYB8IZa5AuE">3 Blue 1 Brown
video</a></li>
</ul>
<h2 id="vector-space">Vector Space</h2>
<p>The vector space defines a set V of vectors, a field F, vector
addition, and scalar multiplication that follow a vector space axioms
such as closure, associativity, identity elements, inverses, and
distributive properties.</p>
<p>For example, ℝ³ is a vector space.</p>
<p>Best described by the [[Basis]]</p>
<h2 id="span">Span</h2>
<p>The span is all the vectors that can be reached by using a linear
combination of a given set of vectors. Adding linearly dependent vectors
does not increase the span.</p>
<h2 id="linear-dependence">Linear Dependence</h2>
<p>A subset of vectors contains the zero vector or one of its vectors
can be represented as a [<a href="#linear-combination">linear
combination</a>] of other vectors. This implies there is a “redundant”
vector in the set. Another definition would be if the null vector 0 can
be obtained through linear combination. <span
class="sidenote-wrapper"><label for="sn-1" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-1" class="margin-toggle"/><span
class="sidenote">Theorem 2 from MoML<br />
<br />
</span></span></p>
<h2 id="linear-independence">Linear Independence</h2>
<p>No vectors in a set can be written as linear combinations other
vectors in the set. Can be found by Gaussian Elimination and checking if
there are no non-zero rows, calculating the determinant for a square
matrix and checking if it is != 0, or if rank = # of vectors. If adding
another vector increases the [<a href="#span">Span</a>] they are
linearly independent. Another definition would be it is linearly
independent iff when the sum of all vectors multiplied by coeffiencts is
zero, all coefficients are zero<span
class="sidenote-wrapper"><label for="sn-2" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-2" class="margin-toggle"/><span
class="sidenote">Theorem 2 from MoML<br />
<br />
</span></span></p>
<h2 id="norm">Norm</h2>
<p>The magnitude of a vector from the origin. Common norms are:</p>
<ul>
<li>L1 (Manhattan): sum of absolute values</li>
<li>L2 (Euclidean): square root of a sum of squares.</li>
</ul>
<p>Norms have three properties:</p>
<ol type="1">
<li>Non-negative</li>
<li>Homogeneity: ||ax|| = ||a|| ||x|| for any scalar a</li>
<li>Triangle inequality = ||x+y|| &lt;= ||x||+||y||</li>
</ol>
<p>Used in: Regularization, Optimization (Grad Descent), Loss functions,
distance metrics, batch and layer normalization.</p>
<h2 id="inner-product">Inner Product</h2>
<ul>
<li>⟨u,v⟩→R</li>
<li>A bilinear mapping that is symmetric and positive definite (always
positive when applied to itself).</li>
<li>Takes two vectors and returns a scalar.</li>
<li>Measures length<span
class="sidenote-wrapper"><label for="sn-3" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-3" class="margin-toggle"/><span
class="sidenote">Distance between vectors for an inner product space (V,
&lt;.,.&gt;) : d(x,y) := |x - y| = sqrt(&lt;x - y&gt;, &lt;x - y&gt;).
Relates to [[L2 Norm]].<br />
<br />
</span></span> and angle<span
class="sidenote-wrapper"><label for="sn-4" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-4" class="margin-toggle"/><span
class="sidenote">cos w = &lt;x, y&gt; / (||x|| ||y||) by Cauchy-Schwartz
Inequality<br />
<br />
</span></span> between them. If ⟨u,v⟩=0, then u⊥v, if ||x|| = 1 = ||y||
it is also orthonormal.</li>
<li>Generalization of [[Dot Product]]</li>
<li>MML pg 73-76</li>
</ul>
<h2 id="determinant">Determinant</h2>
<p>For a 2-D vector space, it gives the change in the area of a square
created by the [[basis vector]]s. Change in volume for a cube in 3D.
When det = 0, it squishes the area down to a line or point, and
indicates the matrix is [[linearly dependent]]. If det is negative,
there is a “flip”, but the change in area is equivalent to the absolute
value of the determinant. <span
class="sidenote-wrapper"><label for="sn-5" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-5" class="margin-toggle"/><span
class="sidenote">Great video from 3b1b on this.<br />
<br />
</span></span></p>
<p>The determinant can only be found for a square matrix.</p>
<p>Calculating the determinant For 2D For 3D For nxn - reduce to 2x2</p>
<h2 id="trace">Trace</h2>
<p>The diagonal sum of the matrix</p>
<h2 id="characteristic-polynomial">Characteristic Polynomial</h2>
<p>For a square matrix: <span class="math display">\[P_a(\lambda) =
det(A - \lambda I)\]</span> Used in calculating [<a
href="#eigenvectors">Eigenvectors</a>]</p>
<h2 id="eigenvectors-and-eigenvalues">Eigenvectors and Eigenvalues</h2>
<ul>
<li>Eigenvectors and values characterize the linear mapping of a square
matrix.</li>
</ul>
<h3 id="eigenvectors">Eigenvectors</h3>
<ul>
<li>Eigenvectors point in the direction of the mapping</li>
</ul>
<h3 id="eigenvalues">Eigenvalues</h3>
<ul>
<li>Eigenvalues indicate how much the eigenvectors are stretched</li>
</ul>
<h3 id="calculations">Calculations</h3>
<ul>
<li>For a matrix A, eigenvector x, and eigenvalue <span
class="math display">\[\lambda\]</span> : <span class="math display">\[
Ax = \lambda x \]</span></li>
<li><span class="math display">\[det(A - \lambda I_n = 0)\]</span></li>
<li>If <span class="math inline">\(A \elem R^(nxn)\)</span> is
symmetric, there is an [[ONB]] of the vector space with the eigenvector
of A and a real eigenvalue</li>
<li>The [<a href="#determinant">Determinant</a>] of a matrix is equal to
the product of its eigenvalues: <span class="math display">\[det(A) =
\Pi i=1 to n \lambda_i\]</span>
<ul>
<li>Ties in the fact that the determinant calculates the area of the
transformation with the eigenvalues.</li>
</ul></li>
<li>Solving for the [<a href="#eigenvalues">Eigenvalues</a>] and [<a
href="#eigenvectors">Eigenvectors</a>]
<ol type="1">
<li>Set [[Charateristic Polynomial]] = 0: <span
class="math display">\[P_A(\lambda) = 0\]</span></li>
<li></li>
</ol></li>
</ul>
<h3 id="pagerank">PageRank</h3>
<ul>
<li>Uses the [[Eigenvector]] of the maximal [<a
href="#eigenvalues">Eigenvalues</a>] to rank a page based on the
incoming links and how important they are.</li>
</ul>
<h2 id="matrix-decomposition">Matrix Decomposition</h2>
<p>Matrix decomposition breaks a matrix down into multiple factors, much
like factoring an equation. The resulting components can describe
characteristics of the matrix, as well as make some calculations more
efficient.</p>
<h3 id="cholesky-decomposition">Cholesky Decomposition</h3>
<p>Decomposes into a lower triangular matrix and its transpose: <span
class="math inline">\(A = LL^T\)</span></p>
<p><span class="math inline">\(\begin{bmatrix} A_{00} A_{01} A_{02} \\
A_{10} A_{11} A_{12} \\ A_{20} A_{21} A_{22} \end{bmatrix}=\)</span>
<span class="math inline">\(\begin{bmatrix} L_{00} L_{01} L_{02} \\
L_{10} L_{11} L_{12} \\ 0 0 0 \end{bmatrix} \begin{bmatrix} L_{00}
L_{01} L_{02} \\ L_{10} L_{11} L_{12} \\ L_{20} L_{21} L_{22}
\end{bmatrix}\)</span> Example:</p>
<h3 id="eigendecomposition">Eigendecomposition</h3>
<p>Breaks a matrix down into <span
class="math inline">\(PDP^(-1)\)</span> but only works on square
matrices.</p>
<h3 id="singular-value-decomposition">Singular Value Decomposition</h3>
<p>Can decompose any matrix into <span class="math inline">\(U \Sigma
V^T\)</span></p>
<h2 id="references">References</h2>
<p>Math for Machine Learning (MML) Math of Machine Learning (MoML)
3b1b</p>
</body>
</html>
