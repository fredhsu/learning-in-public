<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>calculus</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="tufte.css" />
  <script
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
</head>
<body>
<article>
<h1 id="calculus">Calculus</h1>
<section>
<h2 id="hessian-matrix">Hessian Matrix</h2>
<p>A square matrix similar to the Jacobian, but with second order
partial derivatives. It measures the curvature of the function
locally.</p>
<p><span class="math display">\[
(\bold{H}_f)_{i,j}  = \frac{\partial^2 f}{\partial x_i \partial x_j}
\]</span></p>
</section>
<section>
<h2 id="autodiff">Autodiff</h2>
<p>Automatic differentiation (autodiff) is a method for efficiently
computing derivatives of functions expressed as computer programs. Every
program can be broken down into elementary arithmetic operations (add,
multiply, divide, etc.) and common functions (exp, log, sin, cos, â€¦). By
representing these operations in a computational graph and applying the
chain rule, autodiff systematically propagates derivatives through the
graph. Unlike symbolic differentiation (which can produce unwieldy
expressions) or numerical differentiation (which suffers from
approximation error), autodiff provides exact derivatives up to machine
precision. There are two main modes:</p>
<ul>
<li><p>Forward mode: propagates derivatives from inputs to
outputs.</p></li>
<li><p>Reverse mode (<a href="#backpropagation">backpropagation</a>):
propagates derivatives from outputs to inputs, and is particularly
efficient when computing gradients of scalar outputs with respect to
many inputs (as in machine learning).</p></li>
</ul>
</section>
<section>
<h2 id="backpropagation">Backpropagation</h2>
<p>An efficient way to calculate the gradients of the nodes in the
computational graph. It works by calculating the partial derivatives of
each node using the chain rule, starting from the loss node and working
backward from the output to the parameters.</p>
<p>Example:</p>
<p><span class="math display">\[
\frac{\partial L}{\partial \theta_i} =
\frac{\partial L}{\partial f_k} \cdot
\frac{\partial f_k}{\partial f_{k-1}} \cdot
\dots
\frac{\partial f_{i+1}}{\partial f_i} \cdot
\frac{\partial f_{i}}{\partial \theta_i}
\]</span></p>
<p>Backpropagation is a special case of reverse-mode [<a
href="#autodiff">autodiff</a>], using algorithmic as opposed to symbol
methods to calculate the gradients.</p>
</section>
<section>
<h2 id="jacobian-matrix">Jacobian Matrix</h2>
<p>A Jacobian matrix is a collection of all the first-order partial
derivatives of a vector valued function, <span
class="math inline">\(f=\mathbb{R}^n \to \mathbb{R}^m\)</span></p>
<p><span class="math display">\[
J = \nabla_x f = \frac{df(x)}{dx} =
\begin{bmatrix}
\frac{\partial f(x)}{\partial x_1} \ldots
\frac{\partial f(x)}{\partial x_n}
\end{bmatrix} =
\begin{bmatrix}
\frac{\partial f_1}{\partial x_1} &amp; \cdots &amp; \frac{\partial
f_1}{\partial x_n} \\
\frac{\partial f_2}{\partial x_1} &amp; \cdots &amp; \frac{\partial
f_2}{\partial x_n} \\
\vdots &amp; \ddots &amp; \vdots \\
\frac{\partial f_m}{\partial x_1} &amp; \cdots &amp; \frac{\partial
f_m}{\partial x_n}
\end{bmatrix}
\]</span></p>
<p>where</p>
<p><span class="math display">\[
f(x) =
\begin{bmatrix}
f_1(x) \\
f_2(x) \\
\vdots \\
f_m(x)
\end{bmatrix},
\quad
x =
\begin{bmatrix}
x_1 \\
x_2 \\
\vdots \\
x_n
\end{bmatrix}
\]</span></p>
<h3 id="applications">Applications</h3>
<ul>
<li>The determinant of the Jacobian (when <span
class="math inline">\(m=n\)</span>) gives the local volume scaling
factor under the transformation.</li>
<li>Geometrically the Jacobian is the scaling factor when we transform
an area/volume.</li>
</ul>
</section>
<section>
<h2 id="taylor-polynomial">Taylor Polynomial</h2>
<p>Provides an approximation of a function that gets more accurate as
<span class="math inline">\(n\)</span> increases.</p>
<p><span class="math display">\[T_n(x) := \sum_{k=0}^{n}
\frac{f^{(k)}(x_0)}{k!}(x-x_0)^k\]</span> where <span
class="math inline">\(f^{(k)}(x_0)\)</span> is the kth derivative of f
at <span class="math inline">\(x_0\)</span></p>
<p>Taylor series is the Taylor polynomial with <span
class="math inline">\(n=\infty\)</span></p>
</section>
<section>
<h2 id="gradient">Gradient</h2>
<p>A column vector with partial derivatives with regard to the elements
of the original vector.</p>
<p>The gradient of a scalar-valued function <span
class="math inline">\(f:\mathbb{R}^n \to \mathbb{R}\)</span> with
respect to the vector <span class="math inline">\(x =
(x_1,\dots,x_n)\)</span> is the vector of partial derivatives:</p>
<p>$$</p>
_x f(x) =
<span class="math display">\[\begin{bmatrix}
\frac{\partial f}{\partial x_1} \\
\frac{\partial f}{\partial x_2} \\
\vdots \\
\frac{\partial f}{\partial x_n}
\end{bmatrix}\]</span>
<p>^n</p>
<p>$$</p>
<p>It points in the direction of the steepest increase of <span
class="math inline">\(f\)</span>.</p>
</section>
<section>
<h2 id="chain-rule-for-partial-derivatives">Chain rule for partial
derivatives</h2>
<p><span class="math display">\[\frac{\partial}{\partial x}(g \circ
f)(x) = \frac{\partial}{\partial x}(g(f(x))) = \frac{\partial
g}{\partial f} \frac{\partial f}{\partial x}\]</span></p>
<p>This can be applied to parameterized functions to get the [<a
href="#gradient">gradient</a>], for instance <span
class="math inline">\(f(x_1,x_2)\)</span> where <span
class="math inline">\(x_1\)</span> &amp; <span
class="math inline">\(x_2\)</span> are functions of <span
class="math inline">\(t\)</span>, we can apply the chain rule to get the
gradient.</p>
<p>$$</p>
<p> f(x_1(t), x_2(t)) = </p>
<ul>
<li>. $$</li>
</ul>
<p>For multivariable functions use [<a href="#jacobian-matrix">Jacobian
Matrix</a>] Let <span class="math inline">\(f:\mathbb{R}^n \to
\mathbb{R}^m\)</span> and <span class="math inline">\(g:\mathbb{R}^m \to
\mathbb{R}^p\)</span>.<br />
Then:</p>
<p><span class="math display">\[
D(g \circ f)(x) = Dg(f(x)) \cdot Df(x),
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(Df(x)\)</span> is the <span
class="math inline">\(m \times n\)</span> Jacobian of <span
class="math inline">\(f\)</span>,</li>
<li><span class="math inline">\(Dg(f(x))\)</span> is the <span
class="math inline">\(p \times m\)</span> Jacobian of <span
class="math inline">\(g\)</span>,</li>
</ul>
<p>The result is a <span class="math inline">\(p \times n\)</span>
Jacobian.</p>
</section>
</article>
</body>
</html>
